{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c401305b-63c4-4459-aa5f-ba5a7080db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8091c5e3-af80-4428-b4b5-0a7d426c7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available. Falling back to CPU settings.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available. Using GPU settings.\")\n",
    "else:\n",
    "    print(\"GPU not available. Falling back to CPU settings.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "          'MAX_LEN': 128,\n",
    "          'batch_size':8,\n",
    "          'model_name':'NER_model.h5'\n",
    "         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bbb1f4-ab42-4c8c-a11b-75a5376a7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #           Word  POS    Tag\n",
      "0  Sentence: 1      Thousands  NNS      O\n",
      "1          NaN             of   IN      O\n",
      "2          NaN  demonstrators  NNS      O\n",
      "3          NaN           have  VBP      O\n",
      "4          NaN        marched  VBN      O\n",
      "5          NaN        through   IN      O\n",
      "6          NaN         London  NNP  B-geo\n",
      "7          NaN             to   TO      O\n",
      "8          NaN        protest   VB      O\n",
      "9          NaN            the   DT      O\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"ner_dataset.csv\",\n",
    "    delimiter=',',\n",
    "    quoting=3,\n",
    "    encoding='latin1',\n",
    "    names=[\"Sentence #\", \"Word\", \"POS\", \"Tag\"],\n",
    "    skiprows=1,\n",
    "    on_bad_lines='skip'  # Skips malformed rows\n",
    ")\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb7d17a-b54b-4fa2-9bca-da14fc531e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satheeskumar\\AppData\\Local\\Temp\\ipykernel_15756\\3510666491.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0              Sentence: 1\n",
       "1              Sentence: 1\n",
       "2              Sentence: 1\n",
       "3              Sentence: 1\n",
       "4              Sentence: 1\n",
       "                ...       \n",
       "1014292    Sentence: 47959\n",
       "1014293    Sentence: 47959\n",
       "1014294    Sentence: 47959\n",
       "1014295    Sentence: 47959\n",
       "1014296    Sentence: 47959\n",
       "Name: Sentence #, Length: 1014297, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fill sentence numbers\n",
    "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n",
    "\n",
    "df[\"Sentence #\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57959eb8-9d34-4f68-92c2-6d1f8641019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satheeskumar\\AppData\\Local\\Temp\\ipykernel_15756\\1570912286.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sentences = df.groupby(\"Sentence #\").apply(agg_func).tolist()\n"
     ]
    }
   ],
   "source": [
    "# Group sentences\n",
    "agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"Tag\"].values.tolist())]\n",
    "sentences = df.groupby(\"Sentence #\").apply(agg_func).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa5119f-50ae-47a2-bffd-c53bb35bc870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iranian', 'B-gpe'),\n",
       " ('officials', 'O'),\n",
       " ('say', 'O'),\n",
       " ('they', 'O'),\n",
       " ('expect', 'O'),\n",
       " ('to', 'O'),\n",
       " ('get', 'O'),\n",
       " ('access', 'O'),\n",
       " ('to', 'O'),\n",
       " ('sealed', 'O'),\n",
       " ('sensitive', 'O'),\n",
       " ('parts', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('plant', 'O'),\n",
       " ('Wednesday', 'B-tim'),\n",
       " ('after', 'O'),\n",
       " ('an', 'O'),\n",
       " ('IAEA', 'B-org'),\n",
       " ('surveillance', 'O'),\n",
       " ('system', 'O'),\n",
       " ('begins', 'O'),\n",
       " ('functioning', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0792a8-54c9-4e60-885a-cf597d51dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tag vocab\n",
    "unique_tags = list(set(df[\"Tag\"].dropna()))\n",
    "tag2id = {tag: i+1 for i, tag in enumerate(unique_tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "num_labels = len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b49c52-df77-4962-a9be-0ce07bfb6e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-org': 1,\n",
       " 'I-gpe': 2,\n",
       " 'B-tim': 3,\n",
       " 'I-geo': 4,\n",
       " 'I-per': 5,\n",
       " 'I-art': 6,\n",
       " 'B-nat': 7,\n",
       " 'I-eve': 8,\n",
       " 'O': 9,\n",
       " 'I-nat': 10,\n",
       " 'B-org': 11,\n",
       " 'B-eve': 12,\n",
       " 'B-per': 13,\n",
       " 'B-gpe': 14,\n",
       " 'B-art': 15,\n",
       " 'B-geo': 16,\n",
       " 'I-tim': 17}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65d7ac9e-5a3a-4c30-9fde-616df5d3a1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'I-org',\n",
       " 2: 'I-gpe',\n",
       " 3: 'B-tim',\n",
       " 4: 'I-geo',\n",
       " 5: 'I-per',\n",
       " 6: 'I-art',\n",
       " 7: 'B-nat',\n",
       " 8: 'I-eve',\n",
       " 9: 'O',\n",
       " 10: 'I-nat',\n",
       " 11: 'B-org',\n",
       " 12: 'B-eve',\n",
       " 13: 'B-per',\n",
       " 14: 'B-gpe',\n",
       " 15: 'B-art',\n",
       " 16: 'B-geo',\n",
       " 17: 'I-tim'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f7051c-a253-4895-834a-87909710d058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088df0d5-1ff7-4320-bf01-de808860fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b44b5c-e5e8-41b5-b188-81ca560dfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = sorted(set(tag for sent in sentences for _, tag in sent if pd.notnull(tag)))\n",
    "label2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec76f094-eaf3-4abf-b8e0-bc86ca35852e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art': 0,\n",
       " 'B-eve': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-nat': 4,\n",
       " 'B-org': 5,\n",
       " 'B-per': 6,\n",
       " 'B-tim': 7,\n",
       " 'I-art': 8,\n",
       " 'I-eve': 9,\n",
       " 'I-geo': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-nat': 12,\n",
       " 'I-org': 13,\n",
       " 'I-per': 14,\n",
       " 'I-tim': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "75765b65-2e62-43c7-8861-f8147209db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(sentences, label2id, tokenizer):\n",
    "    tokenized_inputs = []\n",
    "    label_ids_list = []\n",
    "\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        sentence = [(w, t) for w, t in sentence if pd.notnull(w) and pd.notnull(t)]\n",
    "\n",
    "        if not sentence:\n",
    "            continue  # skip empty or all-nan sentences\n",
    "\n",
    "        words, tags = zip(*sentence)\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        encoding = tokenizer(\n",
    "            list(words),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt' \n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "\n",
    "        #for word_idx in word_ids:\n",
    "         #   if word_idx is None:\n",
    "          #      label_ids.append(-100)\n",
    "           # else:\n",
    "            #    label_ids.append(label2id.get(tags[word_idx], 0\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(0)  # Padding position\n",
    "            else:\n",
    "                tag = tags[word_idx]\n",
    "                label_ids.append(label2id.get(tag, 0))  # Unknown tags → PAD (0)\n",
    "\n",
    "\n",
    "\n",
    "        tokenized_input = {\n",
    "            \"sentence_id\": f\"Sentence: {idx + 1}\",\n",
    "            \"words\": list(words),\n",
    "            \"tokens\": tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0]),\n",
    "            \"input_ids\": encoding[\"input_ids\"][0].tolist(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"][0].tolist(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"][0].tolist(),\n",
    "            \"offset_mapping\": [tuple(x.tolist()) for x in encoding[\"offset_mapping\"][0]],\n",
    "            \"labels\": label_ids\n",
    "        }\n",
    "\n",
    "\n",
    "        #encoding[\"labels\"] = label_ids\n",
    "        tokenized_inputs.append(tokenized_input)\n",
    "        label_ids_list.append(label_ids)\n",
    "\n",
    "    return tokenized_inputs, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed0bba-8db5-4c81-a820-d020c7aff5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e3695c-f066-4dc1-b7b9-eb1c7b00de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs, label_ids = tokenize_and_align_labels(sentences, label2id, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6c766f-6b80-452b-b8d6-1117255a0cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 'Sentence: 1',\n",
       " 'words': ['Thousands',\n",
       "  'of',\n",
       "  'demonstrators',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'London',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'British',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '.'],\n",
       " 'tokens': ['[CLS]',\n",
       "  'Thousands',\n",
       "  'of',\n",
       "  'demons',\n",
       "  '##tra',\n",
       "  '##tors',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'London',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'British',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'],\n",
       " 'input_ids': [101,\n",
       "  26159,\n",
       "  1104,\n",
       "  8568,\n",
       "  4487,\n",
       "  5067,\n",
       "  1138,\n",
       "  9639,\n",
       "  1194,\n",
       "  1498,\n",
       "  1106,\n",
       "  5641,\n",
       "  1103,\n",
       "  1594,\n",
       "  1107,\n",
       "  5008,\n",
       "  1105,\n",
       "  4555,\n",
       "  1103,\n",
       "  10602,\n",
       "  1104,\n",
       "  1418,\n",
       "  2830,\n",
       "  1121,\n",
       "  1115,\n",
       "  1583,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'offset_mapping': [(0, 0),\n",
       "  (0, 9),\n",
       "  (0, 2),\n",
       "  (0, 6),\n",
       "  (6, 9),\n",
       "  (9, 13),\n",
       "  (0, 4),\n",
       "  (0, 7),\n",
       "  (0, 7),\n",
       "  (0, 6),\n",
       "  (0, 2),\n",
       "  (0, 7),\n",
       "  (0, 3),\n",
       "  (0, 3),\n",
       "  (0, 2),\n",
       "  (0, 4),\n",
       "  (0, 3),\n",
       "  (0, 6),\n",
       "  (0, 3),\n",
       "  (0, 10),\n",
       "  (0, 2),\n",
       "  (0, 7),\n",
       "  (0, 6),\n",
       "  (0, 4),\n",
       "  (0, 4),\n",
       "  (0, 7),\n",
       "  (0, 1),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0)],\n",
       " 'labels': [0,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  2,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  2,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  3,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e49f8f-9556-4090-81b6-e857fabc97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class NERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenized_inputs):\n",
    "        self.data = tokenized_inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.data[idx].items()\n",
    "                if key in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']}\n",
    "        return item\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf1f6056-e7e0-4571-8f2d-b6a06f9553e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NERDataset(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "757fd0dd-81b1-4b7c-808b-24708d5f49fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 26159,  1104,  8568,  4487,  5067,  1138,  9639,  1194,  1498,\n",
       "          1106,  5641,  1103,  1594,  1107,  5008,  1105,  4555,  1103, 10602,\n",
       "          1104,  1418,  2830,  1121,  1115,  1583,   119,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 0, 16, 16, 16, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16, 16,  2, 16, 16,\n",
       "         16, 16, 16,  3, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56af20-67aa-432e-8c65-4685e884cfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0575ef80-1e5c-42ff-9bc7-a58191ebe258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           input_ids  \\\n",
      "0  [101, 26159, 1104, 8568, 4487, 5067, 1138, 963...   \n",
      "1  [101, 7239, 3878, 1474, 1152, 5363, 1106, 1243...   \n",
      "2  [101, 1124, 8031, 4184, 2083, 3832, 12526, 430...   \n",
      "3  [101, 1220, 1286, 1170, 170, 8901, 2396, 118, ...   \n",
      "4  [101, 158, 119, 151, 119, 3893, 13443, 4945, 1...   \n",
      "\n",
      "                                      token_type_ids  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [0, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, 16,...  \n",
      "1  [0, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
      "2  [0, 16, 16, 16, 16, 16, 16, 7, 16, 16, 16, 16,...  \n",
      "3  [0, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
      "4  [0, 2, 2, 2, 2, 16, 16, 6, 14, 14, 14, 16, 7, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert each sample into a list of dicts\n",
    "records = []\n",
    "\n",
    "for item in dataset:  # Assuming dataset is like a list of samples (dicts)\n",
    "    record = {\n",
    "        'input_ids': item['input_ids'].tolist(),\n",
    "        'token_type_ids': item['token_type_ids'].tolist(),\n",
    "        'attention_mask': item['attention_mask'].tolist(),\n",
    "        'labels': item['labels'].tolist()\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Step 2: Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Show result\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea0de4f-79ab-4e45-924e-8bac477695bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 26159, 1104, 8568, 4487, 5067, 1138, 963...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 7239, 3878, 1474, 1152, 5363, 1106, 1243...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 1124, 8031, 4184, 2083, 3832, 12526, 430...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 16, 16, 16, 16, 16, 16, 7, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 1220, 1286, 1170, 170, 8901, 2396, 118, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 158, 119, 151, 119, 3893, 13443, 4945, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 16, 16, 6, 14, 14, 14, 16, 7, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 26159, 1104, 8568, 4487, 5067, 1138, 963...   \n",
       "1  [101, 7239, 3878, 1474, 1152, 5363, 1106, 1243...   \n",
       "2  [101, 1124, 8031, 4184, 2083, 3832, 12526, 430...   \n",
       "3  [101, 1220, 1286, 1170, 170, 8901, 2396, 118, ...   \n",
       "4  [101, 158, 119, 151, 119, 3893, 13443, 4945, 1...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, 16,...  \n",
       "1  [0, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
       "2  [0, 16, 16, 16, 16, 16, 16, 7, 16, 16, 16, 16,...  \n",
       "3  [0, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "4  [0, 2, 2, 2, 2, 16, 16, 6, 14, 14, 14, 16, 7, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f743cf-8152-4f6c-8c51-e6a1121b5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=100)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daee8142-4e4a-4d7f-86bd-0b5dd4dff695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38367, 4), (4796, 4), (4796, 4))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape , test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51dff6e4-df35-4db4-90df-6915626acb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38367, 128) (38367, 128) (38367, 128) (38367, 128)\n",
      "(4796, 128) (4796, 128) (4796, 128) (4796, 128)\n",
      "(4796, 128) (4796, 128) (4796, 128) (4796, 128)\n"
     ]
    }
   ],
   "source": [
    "train_input_ids = np.array(train_data['input_ids'].tolist())\n",
    "train_token_type_ids = np.array(train_data['token_type_ids'].tolist())\n",
    "train_attention_mask = np.array(train_data['attention_mask'].tolist())\n",
    "train_labels = np.array(train_data['labels'].tolist())\n",
    "\n",
    "valid_input_ids = np.array(val_data['input_ids'].tolist())\n",
    "valid_token_type_ids = np.array(val_data['token_type_ids'].tolist())\n",
    "valid_attention_mask = np.array(val_data['attention_mask'].tolist())\n",
    "valid_labels = np.array(val_data['labels'].tolist())\n",
    "\n",
    "test_input_ids = np.array(test_data['input_ids'].tolist())\n",
    "test_token_type_ids = np.array(test_data['token_type_ids'].tolist())\n",
    "test_attention_mask = np.array(test_data['attention_mask'].tolist())\n",
    "test_labels = np.array(test_data['labels'].tolist())\n",
    "\n",
    "print(train_input_ids.shape, train_token_type_ids.shape, train_attention_mask.shape ,train_labels.shape)\n",
    "print(valid_input_ids.shape, valid_token_type_ids.shape, valid_attention_mask.shape ,valid_labels.shape)\n",
    "print(test_input_ids.shape, test_token_type_ids.shape, test_attention_mask.shape ,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df136620-eaaf-4343-951d-c0caff3ab5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = len(tag2id) + 1  # Including PAD class 0\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
    "valid_labels_cat = to_categorical(valid_labels, num_classes=num_classes)\n",
    "test_labels_cat = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "409a3345-a100-42ad-a8bd-dcf5ba8a6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8689766a-af83-4bf0-aa57-53db89fa1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e48eabb-991c-4b44-b647-6ad74047e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForTokenClassification\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "encoder = TFBertForTokenClassification.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "\n",
    "# NER Model\n",
    "input_ids = Input(shape=(config['MAX_LEN'],), dtype=tf.int32)\n",
    "token_type_ids = Input(shape=(config['MAX_LEN'],), dtype=tf.int32)\n",
    "attention_mask = Input(shape=(config['MAX_LEN'],), dtype=tf.int32)\n",
    "\n",
    "embedding = encoder(input_ids,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                   )[0]\n",
    "\n",
    "output_logits = Dense(len(tag2id)+1, activation='softmax')(embedding)\n",
    "\n",
    "model = Model(inputs=[input_ids, token_type_ids, attention_mask],\n",
    "              outputs=[output_logits]\n",
    "             )\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "loss =  CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e2b132-0cff-42a6-b27f-b9a6caea9ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_for_token_classifi  TFTokenClassifierOutput(lo   1077212   ['input_1[0][0]',             \n",
      " cation (TFBertForTokenClas  ss=None, logits=(None, 128   18         'input_3[0][0]',             \n",
      " sification)                 , 2),                                   'input_2[0][0]']             \n",
      "                              hidden_states=((None, 128                                           \n",
      "                             , 768),                                                              \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768),                                                   \n",
      "                              (None, 128, 768)),                                                  \n",
      "                              attentions=None)                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128, 18)              54        ['tf_bert_for_token_classifica\n",
      "                                                                    tion[0][13]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 107721272 (410.92 MB)\n",
      "Trainable params: 107721272 (410.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668f3f4-6abe-4e7d-9515-4d3a9283bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5882999-6141-4d4d-8bd8-b630007e362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4796/4796 [==============================] - ETA: 0s - loss: 1.7106 - accuracy: 0.3336\n",
      "Epoch 1: val_loss improved from inf to 0.14653, saving model to .\\model_512d_001.h5\n",
      "4796/4796 [==============================] - 40184s 8s/step - loss: 1.7106 - accuracy: 0.3336 - val_loss: 0.1465 - val_accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_input_ids,\n",
    "                     train_token_type_ids,\n",
    "                     train_attention_mask],\n",
    "                    train_labels_cat,\n",
    "                    epochs=1,\n",
    "                    validation_data=([valid_input_ids,\n",
    "                                      valid_token_type_ids,\n",
    "                                      valid_attention_mask],\n",
    "                                     valid_labels_cat),\n",
    "                    batch_size=config['batch_size'],\n",
    "                    callbacks=[save_model, tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ad85912-e806-400e-93f6-706461311d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"taskkill /F /IM tensorboard.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e909fc4-887d-4421-8fb8-291ca70cb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b32c06a-1fc5-48c8-a22a-85a079ec95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    profile_batch=5\n",
    ")\n",
    "\n",
    "model_name = \"./model_512d_{epoch:03d}.h5\"\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_name,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50963c28-f306-4eff-b123-51c4d251e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9840), started 1 day, 10:16:21 ago. (Use '!kill 9840' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce937c53231bede6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce937c53231bede6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42570e65-85bc-4fb0-884c-bfe40f547de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15860fd7-f682-4e20-87f9-1abc92de5e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner_model_tokenizer/tokenizer_config.json',\n",
       " 'ner_model_tokenizer/special_tokens_map.json',\n",
       " 'ner_model_tokenizer/vocab.txt',\n",
       " 'ner_model_tokenizer/added_tokens.json',\n",
       " 'ner_model_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.save_pretrained('./my_tokenizer')\n",
    "\n",
    "tokenizer.save_pretrained(\"ner_model_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67b6efa3-4704-456e-a27f-f07906b6520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ner_bert_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ner_bert_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the full model (architecture + weights)\n",
    "model.save(\"ner_bert_model\")  # saves as a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6584cb35-54bb-4269-83da-2a26ff9f7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"model_512d_001.h5\")  # or your best checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23ad3c89-3a36-4035-912d-abf643b84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_ner_model(model,\n",
    "    valid_input_ids,\n",
    "    valid_token_type_ids,\n",
    "    valid_attention_mask,\n",
    "    valid_labels, id2tag):\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict([valid_input_ids, valid_token_type_ids, valid_attention_mask], verbose=1)\n",
    "    y_pred_ids = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "\n",
    "    for i in range(len(valid_labels)):\n",
    "        true_seq = []\n",
    "        pred_seq = []\n",
    "        for j in range(len(valid_labels[i])):\n",
    "            true_id = valid_labels[i][j]\n",
    "            pred_id = y_pred_ids[i][j]\n",
    "            if true_id != 0 and pred_id != 0:\n",
    "                true_seq.append(id2tag[true_id])\n",
    "                pred_seq.append(id2tag[pred_id])\n",
    "\n",
    "        true_tags.append(true_seq)\n",
    "        pred_tags.append(pred_seq)\n",
    "\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(true_tags, pred_tags))\n",
    "    print(f\"Accuracy: {accuracy_score(true_tags, pred_tags):.4f}\")\n",
    "    print(f\"Precision: {precision_score(true_tags, pred_tags):.4f}\")\n",
    "    print(f\"Recall: {recall_score(true_tags, pred_tags):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(true_tags, pred_tags):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dde7bff3-5b27-42a2-8e4c-786f7dfb2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1312s 9s/step\n",
      "\n",
      "Classification Report:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satheeskumar\\apps\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00      2167\n",
      "         eve       0.00      0.00      0.00        21\n",
      "         geo       0.93      0.95      0.94     92545\n",
      "         gpe       0.80      0.36      0.50      7014\n",
      "         nat       0.50      0.18      0.26      2749\n",
      "         org       0.00      0.03      0.00        40\n",
      "         per       0.64      0.65      0.65      3850\n",
      "         tim       0.00      0.00      0.00       162\n",
      "\n",
      "   micro avg       0.89      0.86      0.87    108548\n",
      "   macro avg       0.36      0.27      0.29    108548\n",
      "weighted avg       0.88      0.86      0.86    108548\n",
      "\n",
      "Accuracy: 0.8639\n",
      "Precision: 0.8903\n",
      "Recall: 0.8569\n",
      "F1 Score: 0.8733\n"
     ]
    }
   ],
   "source": [
    "evaluate_ner_model(\n",
    "    model,\n",
    "    valid_input_ids,\n",
    "    valid_token_type_ids,\n",
    "    valid_attention_mask,\n",
    "    valid_labels,\n",
    "    id2tag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cfb29b05-44ea-46f3-b340-6c488d225eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ner_model_tokenizer/\")\n",
    "\n",
    "def ner_inference(text, model, tokenizer, id2tag):\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text.split(), \n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    token_type_ids = encoding[\"token_type_ids\"]\n",
    "\n",
    "    \n",
    "    predictions = model.predict([input_ids, token_type_ids, attention_mask])\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Map tokens back to words\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "    previous_word_idx = None\n",
    "    result = []\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue  # skip special tokens and subwords\n",
    "        predicted_label = id2tag.get(pred_ids[0][idx], \"O\")\n",
    "        word = text.split()[word_idx]\n",
    "        result.append((word, predicted_label))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21b15bfb-639f-4c1b-940f-99e7adb3f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 not in id2tag:\n",
    "    id2tag[0] = 'PAD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c93d61e9-fd9d-40ee-972a-cd93bc8b916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 475ms/step\n",
      "dhoni: I-per\n",
      "visited: B-geo\n",
      "America: I-geo\n"
     ]
    }
   ],
   "source": [
    "text = \"dhoni visited America\"\n",
    "\n",
    "result = ner_inference(text, model, tokenizer, id2tag)\n",
    "\n",
    "for token, tag in result:\n",
    "    print(f\"{token}: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf91c5-afbc-4c22-8c2d-7b94bcbab134",
   "metadata": {},
   "source": [
    "if we run more epochs we get more better scores and tags,  but i didn't have that much effecient lap. i have only cpu lap no gpu supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743c905-0205-4225-9835-c46f980f7063",
   "metadata": {},
   "source": [
    "for single epoch we get this much accuracy if we run 5 to 6 epoch we get 90+ accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7ca67-33a9-4e48-800c-51ce3e2b7fa5",
   "metadata": {},
   "source": [
    "TensorBoard is asked me to restart the kernal so if i do that then i lost everything like model training so only i leave it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dc6a3-bcfa-4ab7-ba4c-4c569672380b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
